import os
import torch
import open_clip
import numpy as np
from tqdm import tqdm

# Define dataset path
DATASET_PATH = r"D:\MTech - IIT Ropar\Subject Lessons\Semester 2\AI\Project\Image DeepFake Detection\WildDeep_Small"

# Load CLIP model
device = "cuda" if torch.cuda.is_available() else "cpu"
model, _, preprocess = open_clip.create_model_and_transforms("ViT-B-32", pretrained="openai", device=device)
tokenizer = open_clip.get_tokenizer("ViT-B-32")

# Function to load a limited number of preprocessed images
def load_preprocessed_images(set_name, label, limit=5):  # Load only 5 per class
    folder = os.path.join(DATASET_PATH, f"processed_{set_name}", label)
    images, filenames = [], []
    for i, file in enumerate(tqdm(os.listdir(folder), desc=f"Loading {set_name}/{label} images")):
        if file.endswith(".pt"):
            img_tensor = torch.load(os.path.join(folder, file)).unsqueeze(0).to(device)
            images.append(img_tensor)
            filenames.append(file)  # Store filenames to track which image got which text
        if len(images) >= limit:  # Stop after loading `limit` images
            break
    return torch.cat(images, dim=0) if images else None, filenames  # Stack all images

# Load a limited number of test images
real_images, real_filenames = load_preprocessed_images("test", "real", limit=5)
fake_images, fake_filenames = load_preprocessed_images("test", "fake", limit=5)

# Define a diverse set of prompts to match against (simulating generated text embeddings)
possible_prompts = [
    "This is a real human face.",
    "This is a person with natural skin.",
    "A genuine, unedited human face.",
    "This is a deepfake image.",
    "An AI-generated face with smooth skin.",
    "A synthetic fake face generated by AI."
]
text_inputs = tokenizer(possible_prompts).to(device)  # Encode multiple prompts

# Function to calculate similarity scores
def get_clip_similarity(images):
    if images is None:
        return None, None
    with torch.no_grad():
        image_features = model.encode_image(images)
        text_features = model.encode_text(text_inputs)
        similarity = (image_features @ text_features.T).softmax(dim=-1)
    return similarity, text_features

# Get similarity scores & text embeddings
real_scores, text_embeddings = get_clip_similarity(real_images)
fake_scores, _ = get_clip_similarity(fake_images)

# Function to find the closest matching prompt based on text embeddings
def find_best_matching_prompt(similarity_scores):
    best_indices = similarity_scores.argmax(dim=-1)  # Find the index of the highest similarity
    return [possible_prompts[i] for i in best_indices.cpu().numpy()]  # Map indices to text prompts

# Get the best-matching text for real and fake images
real_best_prompts = find_best_matching_prompt(real_scores) if real_scores is not None else []
fake_best_prompts = find_best_matching_prompt(fake_scores) if fake_scores is not None else []

# Display results for each image
print("\nğŸ” **Generated Prompts & Classification for Test Images**")
print("\nğŸŸ¢ **Real Images:**")
for filename, prompt, scores in zip(real_filenames, real_best_prompts, real_scores):
    real_prob, fake_prob = scores[0].item(), scores[1].item()
    classification = "âœ… Classified as REAL" if real_prob > fake_prob else "âŒ Misclassified as FAKE"
    print(f"ğŸ“Œ Image: {filename} | ğŸ“ Generated Prompt: {prompt} | ğŸ”¢ Real: {real_prob:.2f}, Fake: {fake_prob:.2f} | {classification}")

print("\nğŸ”´ **Fake Images:**")
for filename, prompt, scores in zip(fake_filenames, fake_best_prompts, fake_scores):
    real_prob, fake_prob = scores[0].item(), scores[1].item()
    classification = "âœ… Classified as FAKE" if fake_prob > real_prob else "âŒ Misclassified as REAL"
    print(f"ğŸ“Œ Image: {filename} | ğŸ“ Generated Prompt: {prompt} | ğŸ”¢ Real: {real_prob:.2f}, Fake: {fake_prob:.2f} | {classification}")
